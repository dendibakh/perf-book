### TMA Summary

TMA is great for identifying CPU performance bottlenecks. Ideally, when we run it on an application, we would like to see the `Retiring` metric at 100%. Although there are exceptions. Having the `Retiring` metric at 100% means a CPU is fully saturated and it crunches instructions at full speed. But it doesn't say anything about the quality of those instructions. A program can spin in a tight loop waiting for a lock; that would show a high `Retiring` metric, but do no useful work. 

Another example in which you might see a high `Retiring` value but slow overall performance is when a program has a hotspot that was not vectorized. You give a processor an "easy" time by letting it run simple non-vectorized operations, but is it an optimal way of using available CPU resources? Of course, no. If a CPU doesn't have problems executing your code, doesn't mean performance cannot be improved. Watch out for such cases and remember that TMA identifies CPU performance bottlenecks but doesn't correlate them with the performance of your program. You will find it out once you do the necessary experiments.

While it is possible to achieve `Retiring` close to 100% on a toy program, real-world applications are far from getting there. Figure @fig:TMA_google shows top-level TMA metrics for Google's datacenter workloads along with several [SPEC CPU2006](http://spec.org/cpu2006/)[^13] benchmarks running on Intel's IvyBridge server processors. We can see that most data center workloads have a very small fraction in the `Retiring` bucket. This implies that most data center workloads spend time stalled on various bottlenecks. `BackendBound` is the primary source of performance issues. The `FrontendBound` category represents a bigger problem for data center workloads than in SPEC2006 because those applications typically have large codebases. Finally, some workloads suffer from branch mispredictions more than others, e.g., `search2` and `445.gobmk`.

![TMA breakdown of Google's datacenter workloads along with several SPEC CPU2006 benchmarks, *© Image from [@GoogleProfiling]*](../../img/pmu-features/TMA_google.jpg){#fig:TMA_google width=80%}

Keep in mind that the numbers are likely to change for other CPU generations as architects constantly try to improve the CPU design. The numbers are also likely to change for other instruction set architectures (ISA) and compiler versions.

A few final thoughts before we move on... Using TMA on a code that has major performance flaws is not recommended because it will likely steer you in the wrong direction, and instead of fixing real high-level performance problems, you will be tuning bad code, which is just a waste of time. Similarly, make sure the environment doesn’t get in the way of profiling. For example, if you drop the filesystem cache and run the benchmark under TMA, it will likely show that your application is Memory Bound, which in fact, may be false when the filesystem cache is warmed up.

Workload characterization provided by TMA can increase the scope of potential optimizations beyond source code. For example, if an application is bound by memory bandwidth and all possible ways to speed it up on the software level have been exhausted, it may be possible to improve performance by upgrading the memory subsystem with faster memory chips. This illustrates how using TMA to diagnose performance bottlenecks can support your decision to spend money on new hardware.

[^13]: SPEC CPU 2006 - [http://spec.org/cpu2006/](http://spec.org/cpu2006/).
