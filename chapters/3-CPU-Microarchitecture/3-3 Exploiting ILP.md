## Exploiting Instruction Level Parallelism (ILP)

Most instructions in a program lend themselves to be pipelined and executed in parallel, as they are independent. Modern CPUs implement a large menu of additional hardware features to exploit such instruction-level parallelism (ILP); parallelism within a single stream of instructions. Working in concert with advanced compiler techniques, these hardware features provide significant performance improvements. 

### Out-of-order Execution (OoOE)

The pipeline example in Figure @fig:Pipelining shows all instructions moving through the different stages of the pipeline in-order, i.e., in the same order as they appear in the program (program order). Most modern CPUs support out-of-order Execution (OoOE), i.e., sequential instructions can enter the execution pipeline stage in any arbitrary order only limited by their dependencies and the available resources. OOoE CPUs must still give the same result as if all instructions were executed in the program order (not true: on ARM the OOoE of loads/stores is allowed to become visible.. loads for sure. So it depends on the memory model of the ISA). 

An instruction is called *retired* after it is finally executed, and its results are correct and visible in the [architectural state](https://en.wikipedia.org/wiki/Architectural_state). To ensure correctness, CPUs must retire all instructions in the program order. OOoE is primarily used to avoid underutilization of CPU resources due to stalls caused by dependencies, especially in superscalar engines described in the next section. 

Scheduling of these instructions can be done at compiletime, which is called static scheduling, or done at runtime, which is called dynamic scheduling.

#### Static scheduling

The Intel Itanium which was meant as a replacement for the X86, is an example of static scheduling. With static scheduling of a superscalar, multi-execution unit machine, the scheduling is moved from the hardware to the compiler using a technique known as VLIW - Very Long Instruction Word. The rationale is to simplify the hardware by requiring the compiler to choose the right mix of instructions to keep the machine fully utilized. Compilers can use techniques such as software pipelining and loop unrolling to look farther ahead than can be reasonably supported by hardware structures to find the right ILP. 

The Intel Itanium (also nicknamed Itanic) never managed to become a success:
- The IA-64 is not backwards compatible with IA-32 (X86) code. 
- very difficult to schedule the instructions in such a way to keep the CPU busy due to variable load latencies.
The 64 bit extension of the X86, X86_64, was introduced in the same time window by AMD and was compatible with X86 and eventually became the real successor of the X86. The last Intel Itanium processors where shipped in 2021.

#### Dynamic scheduling

To overcome the problem with static scheduling, modern processors use dynamic scheduling. 2 Important algorithms for dynamic scheduling algorithms are:
- [Scoreboading](https://en.wikipedia.org/wiki/Scoreboarding),[^4] 
- [Tomasulo algorithm](https://en.wikipedia.org/wiki/Tomasulo_algorithm),[^5]. 

The main drawback of scoreboarding is that it will not only preserve true dependencies (RAW), but also name dependencies (WAW and WAR); and therefor provides suboptimal ILP. Name dependencies are caused by the small number of physical registers (typically between 16 and 32 on modern processors).

That is why all modern processors have adopted the Tomasulo algoritm for dynamic scheduling. Tomasolu algorithm was invented in 1960s by Robert Tomasulo and first implemented in the IBM360 model 91. 

To only preserve RAW, Tomasulo makes use of a technique called register renaming by mapping the very limited number of architectural registers, to a much larger set of physical registers (hundreds). Because name dependencies (WAW/WAR) are not preserved, ILP is improved compared to scoreboarding.

An example of register renaming:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(1) AR_1=10
(2) AR_2=AR_1 ;; RAW
(3) AR_2=20   ;; WAW
(4) AR_1=30   ;; WAR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
after register renaming:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(1) PR_35=10
(2) PR_36=PR_35 ;; RAW
(2) PR_37=20
(3) PR_38=30
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the above example, only the RAW dependency (2) is preserved. The above instruction can be performed in any order as long as (1) is ordered before (2). It depends on the ISA memory order which reordering is allowed to become architecturally visible.

A dependency chain is when you have a chain of RAW dependencies, e.g.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(1) AR_1=10
(2) AR_2=AR_1 ;; RAW
(3) AR_3=AR_2 ;; RAW
(4) AR_4=AR_3 ;; RAW
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This is problematic for OoOE because there no increase in ILP after register renaming since all the RAW dependencies are preserved:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
(1) PR_35=10
(2) PR_36=AR_35 ;; RAW
(3) PR_37=AR_36 ;; RAW
(4) PR_38=AR_37 ;; RAW
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dependency chains are often found in loops (loop carried dependency) where one instruction depends on the outcome of the previous iteration. 

It gets worse if the loads miss in the cache because the CPU could stall even though the process remains scheduled on the core; so you get a misleading high CPU utilization. That is why it is good to check the instructions per cycle (IPC) to see if the CPU is doing anything useful. When there are multiple independent dependency chains, then there is still a chance that the CPU can find ILP. Techniques like loop unrolling can help to increase the ILP.

The OoOE in the Tomasulo algorithm is implemented using reservation stations where instructions wait for their input operands to become available. Once these are available, the instructions can be dispatched to the approproate execution unit. So instructions can be executed in any order once their operands become available and are not tied to the program order any longer.

Most implementations strive to balance the hardware cost with the potential return. Typically, the size of the reorder buffer determines how far ahead the hardware can look for scheduling such independent instructions. With the introduction of the Apple M1, the reorder buffer size almost doubled compared to the fastest contemporary X86 implementations and Apple showed there is a lot of potential to uncover more parallel instructions inside a singe instruction stream. Modern processors will be wider (more execution units) and deeper (larger rob).

![The concept of out-of-order execution.](../../img/uarch/OOO.png){#fig:OOO width=80%}

Figure @fig:OOO details the concept underlying out-of-order execution with an example. Assume instruction x+1 cannot execute in cycles 4 and 5 due to a conflict. An in-order CPU would stall all subsequent instructions from entering the EXE pipeline stage. In a CPU with OOO execution, a subsequent instruction that does not have any conflicts (e.g., instruction `x+2`) can enter and complete its execution. All instructions still retire in order, i.e., the instructions complete the WB stage in the program order.

### Superscalar Engines

Most modern CPUs are superscalar i.e., they can issue more than one instruction in a given cycle. Issue-width is the maximum number of instructions that can be issued during the same cycle. Typical issue-width of current generation CPUs ranges from 2 to 6. To ensure the right balance, such superscalar engines also have more than one execution unit and/or pipelined execution units. CPUs also combine superscalar capability with deep pipelines and out-of-order execution to extract the maximum ILP for a given piece of software. 

![The pipeline diagram for a simple 2-way superscalar CPU.](../../img/uarch/SuperScalar.png){#fig:SuperScalar width=55%}

Figure @fig:SuperScalar shows an example CPU that supports 2-wide issue width, i.e., in each cycle, two instructions are processed in each stage of the pipeline. Superscalar CPUs typically support multiple, independent execution units to keep the instructions in the pipeline flowing through without conflicts. In addition to pipelining, replicating execution units further increases the performance of a machine.

### Speculative Execution {#sec:SpeculativeExec}

As noted in the previous section, control hazards can cause significant performance loss in a pipeline if instructions are stalled until the branch condition is resolved. One technique to avoid this performance loss is hardware branch prediction logic to predict the likely direction of branches and allow executing instructions from the predicted path (speculative execution).

Let's consider the short code example in @lst:Speculative. For a processor to understand which function it should execute next, it should know whether the condition `a < b` is false or true. Without knowing that, the CPU waits until the result of the branch instruction will be determined, as shown in Figure @fig:NoSpeculation. 

Listing: Speculative execution

~~~~ {#lst:Speculative .cpp}
if (a < b)
  foo();
else
  bar();
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

<div id="fig:Speculative">
![No speculation](../../img/uarch/Speculative1.png){#fig:NoSpeculation width=60%}


![Speculative execution](../../img/uarch/Speculative2.png){#fig:SpeculativeExec width=60%}

The concept of speculative execution.
</div>

With speculative execution, the CPU takes a guess on an outcome of the branch and initiates processing instructions from the chosen path. Suppose a processor predicted that condition `a < b` will be evaluated as true. It proceeded without waiting for the branch outcome and speculatively called function `foo` (see Figure @fig:SpeculativeExec, speculative work is marked with `*`). State changes to the machine cannot be committed until the condition is resolved to ensure that the architectural state of the machine is never impacted by speculatively executing instructions. In the example above, the branch instruction compares two scalar values, which is fast. But in reality, a branch instruction can be dependent on a value loaded from memory, which can take hundreds of cycles. If the prediction turns out to be correct, it saves a lot of cycles. However, sometimes the prediction is incorrect, and the function `bar` should be called instead. In such a case, the results from the speculative execution must be squashed and thrown away. This is called the branch misprediction penalty, which we will discuss in [@sec:BbMisp].

Speculative execution is added to the Tomasulo algorithm using the reorder buffer (ROB). The ROB is a circular buffer keeps track of the state of each instructions including the speculative state. Instructions in the ROB are issued in program order, can execute out of order, and retire in order. Register renaming is done when the instructions are placed in the ROB. Instructions from the ROB are placed in reservation stations, where they wait for the operands to be ready. And then they can be send to the appropriate execution unit.

Once the instructions are not speculative any longer, the instructions can retire. Here is where the architectural state is comitted; including the updating of the architectural registers. 

When there is a speculation failure due to a misprediction, because the instruction where the speculation happened and later instructions, have not comitted, it is easy to roll back. It depends on the microarchitecture if the rollback happens on all speculative state of the ROB, or only till the instruction where the speculation failed. The latter makes the misprediction a lot less expensive.

Even though modern processors retire the ROB in order, it doesn't mean that OoOE execution can't be observed. Loads could be performed out of order, and even though stores typically end up in a store buffer, the stores don't need to be comitted to the coherent cache in order. It depends on the memory model of the ISA which reorderings are allowed to become architecturally visible.

It isn't only branches the CPU can speculate on. Another example is speculating on reordering of loads in case of the X86. According to the TSO memory model, the memory model of the X86, loads need to be performed in program order. But due to OoOE, they can execute out of order which will help to prevent stalls due to cache misses. When the core detect that that a store issued by anther core, could potentially lead to a violation of TSO, a speculation failure is triggered [^7]. 

Another feature provided by the ROB is precise exceptions. Instructions can run into exceptions e.g. division by zero. Because instructions can execute out of order, it could happen that a later instruction is executed before an earlier exceptional instruction. But the architectural state of the later instruction should not become visible. When an instruction runs into an exception, this is recorded into the ROB entry for that instruction. When that instruction finally retires, the exception state is detected and this trigger flushing all later instructions from the ROB and then the exception handler is called. So is perfectly clear where the exception occurred and no architecural state after the exception will become visible.

### Branch Prediction

As we just have seen, correct predictions greatly improve execution as they allow a CPU to make forward progress without having results of previous instructions available. However, bad speculation often incurs costly performance penalties. Modern CPUs employ sophisticated dynamic branch prediction mechanisms that provide very high accuracy and can adapt to dynamic changes in branch behavior. There are three types of branches which could be handled in a special way:

* **Unconditional jumps and direct calls**: they are the easiest to predict as they are always taken and go in the same direction every time.
* **Conditional branches**: they have two potential outcomes: taken or not taken. Taken branches can go forward or backward. Forward conditional branches are usually generated for `if-else` statements, which have a high chance of not being taken, as frequently it represents error-checking code. Backward conditional jumps are frequently seen in loops and are used to go to the next iteration of a loop; such branches are usually taken.
* **Indirect calls and jumps**: they have many targets. An indirect jump or indirect call can be generated for a `switch` statement, a function pointer, or a `virtual` function. A return from a function deserves attention because it has many potential targets as well.

Most prediction algorithms are based on previous outcomes of the branch. The core of the branch prediction unit (BPU) is a branch target buffer (BTB), which caches the target addresses for every branch. Prediction algorithms consult the BTB every cycle to generate the next address from which to fetch instructions. The CPU uses that new address to fetch the next block of instructions. If no branches are identified in the current fetch block, the next address to fetch will be the next sequential aligned fetch block (fall through). 

Unconditional branches do not require prediction; we just need to lookup the target address in the BTB. Remember, every cycle the BPU needs to generate the next address from which to fetch instructions to avoid pipeline stalls. We could have extracted the address just from the instruction encoding itself, but then we have to wait until the decode stage is over, which will introduce a bubble in the pipeline and make things slower. So, the next fetch address has to be determined at the time when the branch is fetched. 

For conditional branches, we first need to predict whether the branch will be taken or not. If it is not taken, then we fall through and there is no need to lookup the target. Otherwise, we lookup the target address in the BTB. Conditional branches usually account for the biggest portion of total branches and are the main source of misprediction penalties in production software. For indirect branches we need to select one of the possible targets, but the prediction algorithm can be very similar to conditional branches.

All prediction mechanisms try to exploit two important principles, which are similar to what we will discuss with caches later:

* **Temporal correlation**: the way a branch resolves may be a good predictor of the way it will resolve at the next execution. This is also know as local correlation.
* **Spatial correlation**: several adjacent branches may resolve in a highly correlated manner (a preferred path of execution). This is also know as global correlation.

The best accuracy is often achieved by leveraging local and global correlation together. So, not only do we look at the outcome history of the current branch, we also correlate it with the outcomes of other branches. 

Another common technique used is called hybrid prediction. The idea is that some branches have biased behavior. For example, if a conditional branch goes in one direction 99.9% of the time, there is no need to use a complex predictor and pollute its data structures. A much simpler machanism can be used instead. Another example is a loop branch. If a branch has loop behavior, then it can be predicted using a dedicated loop predictor, which will remember the number of iterations the loop typically executes.

Today, state of the art prediction is dominated by TAGE-like [@Seznec2006] or perceptron-based [@Jimenez2001] predictors. Championship[^6] branch predictors make less than 3 mispredictions per 1000 instructions. Modern CPUs routinely reach >95% prediction rate on most workloads.

[^4]: Scoreboarding - [https://en.wikipedia.org/wiki/Scoreboarding](https://en.wikipedia.org/wiki/Scoreboarding).
[^5]: Tomasulo algorithm - [https://en.wikipedia.org/wiki/Tomasulo_algorithm](https://en.wikipedia.org/wiki/Tomasulo_algorithm).
[^6]: 5th Championship Branch Prediction competition - [https://jilp.org/cbp2016](https://jilp.org/cbp2016).
[^7]: Machine Clears - [https://portal.nacad.ufrj.br/online/intel/vtune2017/help/GUID-F0FD7660-58B5-4B5D-AA9A-E1AF21DDCA0E.html](https://portal.nacad.ufrj.br/online/intel/vtune2017/help/GUID-F0FD7660-58B5-4B5D-AA9A-E1AF21DDCA0E.html).