## Active Benchmarking

As you have seen in the previous sections, measuring performance is a complex task with many pitfalls along the way. As human beings, we tend to welcome favorable results and ignore unfavorable ones. This often leads to benchmarking done in a "run and forget" style, with no additional analysis, and overlooking any potential problems. Measurements done in this way are likely incomplete, misleading, or even erroneous. Consider the following two scenarios:

* Developer A on a team meeting: "If we add the `final` keyword to the class declaration across our entire C++ codebase, it will make our code 5% faster, with some tests showing up to 30% speedup."
* Developer B on the next team meeting: "I looked closely at the performance impact of adding the `final` keyword to the class declarations. First, I performed longer tests and haven't measured speedups larger than 5%. The initially observed 30% speedups were outliers caused by test instability. I also noticed that the two machines used for measurements have different configurations: while the CPUs are identical, one of the machines has DRAM memory with larger capacity and lower latency. I ran the tests on the same machine and observed a performance difference within 1%. I compared the generated machine code before and after the change and found no significant differences. Also, I compared the number of instructions executed, cache misses, page faults, context switches, etc., and haven't found any anomalies. At this point, we concluded that the performance impact of the `final` keyword is negligible compared to other optimizations we could make."

Benchmarking done by developer A was done in a passive way. The results were presented without any technical explanation, and the performance impact was exaggerated. In contrast, developer B performed *active benchmarking*.[^1] She ensured proper machine configuration, ran extensive testing, looked one level deeper, and collected as many metrics as possible to support her conclusions. Her analysis explains the underlying technical reason for performance results she observed.

You should have a good intuition to spot suspicious benchmark results. Whenever you see publications that present benchmark results that look too good to be true and without any technical explanation, you should be skeptical. There is nothing wrong with presenting results of your measurements, but as John Ousterhout said, "Performance measurements should be considered guilty until proven innocent." [@MeasureOneLevelDeeper] The best way to verify the results is through active benchmarking. Active benchmarking requires much more efforts than passive benchmarking, but it is the only way to get reliable results.

[^1]: A term coined by Brendan Gregg - [https://www.brendangregg.com/activebenchmarking.html](https://www.brendangregg.com/activebenchmarking.html).