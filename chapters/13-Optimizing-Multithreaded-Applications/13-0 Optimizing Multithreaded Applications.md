# Optimizing Multithreaded Applications {#sec:secOptMTApps}

Modern CPUs are getting more and more cores each year. As of 2024, you can buy a server processor which will have more than 200 cores! And even a mid-range laptop with 16 execution threads is a pretty usual setup nowadays. Since there is so much processing power in every CPU, effective utilization of all the HW threads becomes more challenging. Preparing software to scale well with a growing amount of CPU cores is very important for the future success of your application.

There is a difference in how server and client products exploit parallelism. Most server platforms are designed to process requests from a large number of customers. Those requests are usually independent of each other, so the server can process them in parallel. If there is enough load on the system, applications themselves could even be single-threaded, and still platform utilization will be high. The situation changes drastically, if you start using your server for HPC or AI computations; then you need all the computing power you have. On the other hand, client platforms, such as laptops and desktops, have all the resources to serve a single user. In this case, an application has to make use of all the available cores to provide the best user experience.

Multithreaded applications have their specifics. Certain assumptions of single-threaded execution become invalid when we start dealing with multiple threads. For example, we can no longer identify hotspots by looking at a single thread since each thread might have its hotspot. In a popular [producer-consumer](https://en.wikipedia.org/wiki/Producerâ€“consumer_problem)[^5] design, the producer thread may sleep most of the time. Profiling such a thread won't shed light on the reason why a multithreaded application is performing poorly. We must concentrate on the critical path that has an impact on the overall performance of the application.

There are two main ways to achieve software parallelism: multiprocessing and multithreading. In a multiprocess application, multiple independent processes run concurrently. Each process has its own memory space and communicates with other processes through inter-process communication (IPC) mechanisms such as pipes, sockets, or shared memory. In a multithreaded application, a single process contains multiple threads, which share the same memory space and resources of the process. Threads within the same process can communicate and share data more easily because they have direct access to the same memory space. However, synchronization between threads is usually more complex and is prone to issues like race conditions and deadlocks. In this chapter we will mostly focus on multithreaded applications, however, some techniques can be applied to multiprocess applications as well. We will show examples of both types of applications in this chapter.

When talking about throughput-oriented applications, we can distinguish the following two types of applications:

* **Massively parallel applications**. This type of application usually scales well with the number of cores. They are designed to process a large number of independent tasks. Massively parallel programs often use the divide-and-conquer technique to split the work into smaller tasks (also called *worker threads*) and process them in parallel. Examples of such applications are scientific computations, video rendering, data analytics, AI, and many others. The main obstacle for this type of application is the saturation of a shared resource, such as memory bandwidth, that can effectively stall all the worker threads in the process.
* **Applications that require synchronization**. This type of application has workers share resources to complete their tasks. Worker threads depend on each other, which creates periods when some threads are stalled. Examples of such applications are databases, web servers, and other server applications. The main challenge in this type of application is to minimize required synchronization and to avoid contention on shared resources.

In this chapter, we will explore how to analyze the performance of both types of applications. Since this is a book about low-level performance, we will not discuss algorithm-level optimizations such as lock-free data structures, which are well covered in other books.

[^5]: Producer-consumer pattern - [https://en.wikipedia.org/wiki/Producer-consumer_problem](https://en.wikipedia.org/wiki/Producer-consumer_problem)
